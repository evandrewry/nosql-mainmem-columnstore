\documentclass[11pt]{article}
\pagestyle{plain}
\usepackage[left=1in, right=1in]{geometry}
\begin{document}
\subsection{Vendor Analysis}
The wide difference between certain vendors' target market is testament to the applicability of main-memory databases. While each vendor has different approaches to implementing main-memory databases, they all take advantage of the superior performance of modern CPUs to improve database system throughput. The following is an overview of each vendor.

\subsubsection{IBM's Blink}
Blink is an main-memory database system built for ad-hoc querying over massive fact-tables.\cite{inablink} The key features Blink provides include row compression that uses a specialized dictionary encoding method, and a complex scan-only query executor that operates on compressed data. In comparison to a majority of traditional RDBMS, Blink does not have a performance layer that uses indexes or cached materializations to improve query response time since it can quickly scan all data in RAM. Blink's method of compression, titled \textit{frequency partitioning}, involves partitioning the columns of a table based on data frequency; afterwards, each partition is compressed using a dictionary-encoding scheme with dictionaries built for each partition. \cite{inablink} Blink claims that this compression scheme is better than column-store compression because column-stores have fixed-sized columns and must therefore pad data on the page, whereas Blink's dictionaries minimize the padding in each partition.\cite{inablink} Blink has a unique process of executing queries since Blink has no indexes and therefore has no need to perform access-plan optimizations. \cite{inablink} In practice, Blink has found success as a database \textit{accelerator} rather than a stand-alone database system. \cite{inablink} As an \textit{accelerator}, Blink allows traditional disk-based database systems to bulk-load tables into a Blink cluster in order to route ad-hoc queries to Blink for real-time analytic performance. \cite{inablink} As an accelerator, Blink claims to achieve 60x to 1400x decrease in query response time on a retail fact-table containing over 1 billion rows, and shows a near-uniform response time between 2-4 seconds in comparison to disk-based data warehouse \cite{inablink} While these speedups are impressive, one must keep in mind that Blink has to be loaded with the data first, which may take considerable time.

\subsubsection{H-Store}
H-Store is a heavily-distributed OLTP database that keeps all data permanently in main-memory across multiple physical computers. \cite{kallman2008h} H-Store's focus on transaction processing is significantly different than Blink's focus on ad-hoc queries, though both architectures are centered around storing data in main-memory for performance reasons. Unlike Blink's lack of indexes and query-optimization, H-Store utilizes indexes for access-cost optimization and implements many layers of query planning and optimizing both in deployment and runtime. \cite{kallman2008h} The driving aspect behind H-Store is the use of distributed clusters to divide the storage of large OLTP databases across multiple servers which keep that data available in main memory. \cite{kallman2008h, harizopoulos2008oltp} Since OLTPs are likely to have few distinct transaction procedures, H-Store clusters are deployed with administrator-defined transaction procedures to improve query optimization and evaluation time. \cite{kallman2008h} Once H-Stores are running, it may receive an SQL query via its API and sends the request to a transaction manager which serializes the query over multiple nodes and returns the result.\cite{kallman2008h} The developers behind H-Store have shown that significant OLTP bottlenecks such as locking, logging and buffer management can account for up to 25\% of the overall instructions needed to execute a query. \cite{harizopoulos2008oltp} In order to overcome the logging bottleneck while still maintaining full availability, H-Store avoids implementing REDO logs in favor for state-restoration using duplicates databases that can take-over in case of failures.\cite{harizopoulos2008oltp} As for locking, H-Store avoids multi-threading in favor for a single-threaded engines that treat each processor in a multi-processor server as its own node.\cite{kallman2008h} 

\subsubsection{Exasol's EXASolution}
Exasol's EXASolution is an main-memory database designed for analytics, incorporating the storage paradigm of a column-oriented database with the scalability of distributed clusters. \cite{exasolwp} Like Blink, EXASolution was designed with business intelligence applications in mind, yet unlike Blink, EXASolution has found successes as a standalone database management system.\cite{exasolwp} EXASolution's general query execution strategy is to have multiple, shared-nothing nodes process a small part of the query locally, returning the results on a user-request basis. \cite{exasolwp} Since EXASolution is column-oriented, it heavily compresses its data due to the nature of column-store blocks having highly similar data - a subject discussed in more detail in the next section. Another key feature EXASolution provides is a collection of "intelligent" main-memory algorithms that Exasol claims are capable of accessing data in nanoseconds, though they leave the details of these algorithms out of their technical literature. \cite{exasolwp} Unlike H-Store, which stores data entirely on RAM, each EXASolution node writes data out to a local hard-diss, providing each node with the ability to recover from failure using a local raid technology. \cite{exasolwp} Exasol claims that EXASolution is the "fastest and most scalable database in the world"\cite{exasolws}, which is clearly an exaggerated statement considering how it is not intended to be a general-purpose RDBMS, yet EXASolution's TPC-H benchmark performances are indeed impressive.\cite{exasolws} In every size category, EXASolution outperformed the competition by considerable margins. \cite{exasolws}

\subsubsection{RAMcloud}
RAMcloud, built at Stanford, is a high-performance main-memory storage system that distributes data across thousands of servers, storing all data in DRAM for fast access.\cite{ramcloudpp, ousterhout2010case} Unlike EXASolution, Blink and H-Store, RAMcloud is designed as a replacement for traditional disk-based relational databases that do not scale well for growing web applications, seeking to reduce random-access data latency that comes with retrieving data from a traditional disk-bound storage server. \cite{ousterhout2010case} RAMcloud boasts a uniform 5-10 $\mu$s data latency, however this is only within a specialized, highly tuned network, with actual data latency ranging between 300-500$\mu$s over standard ethernet/TCP networks. \cite{ousterhout2010case} In order to achieve these results, RAMcloud distributes data across 1000 to 10000 servers that have between 32-256 GB of DRAM to store data. \cite{ramcloudpp} In terms of durability, RAMcloud provides a solution that combines DRAM replicas and writing to a persistent disk called \textit{buffer logging}.\cite{ousterhout2010case} This approach is not as expensive as keeping multiple copies of each DRAM, but must sacrifice disk bandwidth in order to maintain DRAM-like read and writes, reducing overall system throughput. \cite{ousterhout2010case, ramcloudpp} As well, RAMcloud's data-model is unique in that is represents data using indexable key-value pairs that stores unstructured data, allowing servers to be data-structure agnostic, increasing scalability. \cite{ousterhout2010case, ramcloudpp} 

\subsection{Analysis}
Just as we have seen the rise and decline of CD-ROMs, cassette tapes, and single-core CPUs as they were replaced by smaller, faster, and generally better alternatives, we should also expect to see the same thing happen to magnetic disks, and even solid state drives as the tech industry continues to advance. It seems like a logical next step for database management systems to move towards in-memory data persistence as the demand for real-time data processing explodes and the cost of RAM continues to decline. However, this transition (if it happens at all) will not happen overnight. The current state of RAM simply does not allow massive quantities of data to be stored as in the data warehouses of companies like Google, Facebook, and Amazon. And, in fact, storing all of this data in RAM probably wouldn’t even result in much of a performance boost since such a large amount of it is seldom accessed at all. We instead expect to see a general push towards hybrid database systems, with in-memory optimizations being integrated into systems that are currently only disk-resident. This could also be achieved by pairing traditional database systems with main-memory ones, similar to what TimesTen does when paired with Oracle. Eventually, we expect that any good database system will be able to recognize the chunks of data that are frequently accessed and know to keep them permanently resident in RAM. These systems will use the optimization techniques pioneered by the main-memory databases of today when manipulating the frequently accessed in-memory data items, and pair these techniques with the well-studied algorithms for manipulating data on disk that already exist in traditional relational databases.

\subsubsection{Is it important?}
Building main-memory database systems has been a beneficial challenge in the database research community, as the impressive data access times of RAM have inspired innovative ways to make RAM as ACID-complaint as possible. Moreover, it has spurred researchers to identify and quantify new bottlenecks in main-memory query execution in order to amortize query response time. \cite{ross2004selection} Main-memory databases have also expanded the use of column-store technology by incorporating many of the techniques found in column-stores in order to further improve system throughput.\cite{exasolwp} On a larger scale, globalization has been pushing the demand for services availability, making distributed in-memory databases more appealing to companies that wish to provide robust service to customers around the globe.\cite{gartnerimdb2} In relation to cloud-computing, in-memory databases can drive cloud-computing services by providing a very scalable storage solution. \cite{ousterhout2010case} It seems that nearly any industry can find ways to utilize the speed of main-memory databases to improve productivity.

\subsubsection{Can the benefits be quantified?}
Main-memory databases have found their way onto TPC benchmarks, most notably TPC-H, where EXASolution leads most of the competition in queries per hour. \cite{tpchbm} While we can easily point to fast random access times as the major benefit of main-memory databases, it seems that many of the other benefits are derived from other emerging technologies such as column-stores and MapReduce databases. Recall that IBM Blink was originally developed to be a standalone database, but has so far only been used as an analytics accelerator for more robust databases, and has future plans to become a more column-oriented database that does not enforce that all data be kept in main memory.\cite{inablink}

\subsubsection{Mature products?}
Main-memory databases are among the most highly developed and researched alternative database solutions, and also among the oldest. So, there are a number of mature products available, both open-source and proprietary. Once again we can refer to EXASolution’s impressive TCP-H benchmarks to argue that there are mature main-memory databases that successfully demonstrate the performance benefits.\cite{tpchbm}

\subsubsection{Should I invest?}
While we believe main-memory databases will be replaced by more cache-conscious, robust in-disk databases, the hype surrounding main-memory databases is enough to justify an investment. It seems that as long as CPU performance continues to improve, there will be more main-memory databases thrown into the market boasting novel main-memory algorithms that take advantage of the latest CPU performance features. Moreover, the wide applicability of main-memory database systems makes it a safer investment.

\pagebreak
\bibliographystyle{plain}
\bibliography{part1c}
\end{document}