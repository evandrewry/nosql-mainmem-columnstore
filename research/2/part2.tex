\documentclass[11pt]{article}
\pagestyle{plain}
\usepackage[left=1in, right=1in]{geometry}
\begin{document}
\section{Column-store Databases}
The paradigm of row-oriented database systems has faced several performance barriers in recent years as businesses and research facilities have become interested in performing more ad-hoc queries in less time. As CPU processing speeds have continued to rise, disk IO latency still remains a critical bottleneck for many database systems.\cite{stonebraker2005c} One proposed solution for improving read performance is a new architectural design referred to as column-oriented database systems, or "column-store" for short. Column-store, in a nutshell, stores the attribute values for a column into disk pages as opposed to row-stores which store whole tuples into disk pages. Moreover, column-stores are particularly good at compressing data and working with compressed data to execute queries, allowing businesses to store more data on disk, therefore saving money.\cite{abadi2006integrating} The ability to read only the columns of interest from a database has piqued the interest of data warehouse, decision support, scientific data-mining and business intelligence application developers due to their ability to avoid reading in data unassociated with the current query.\cite{harizopoulos2006performance, abadi2009column} Researchers have been picking apart the components of classic column-store databases such as MonetDB and C-store in order to determine whether the benefits of column-store can be implemented in a row-store architecture with minimal consequences.\cite{harizopoulos2006performance, bruno2009teaching,holloway2008read}

\subsection{History and Motivation}
Before column-store, the NSM storage model, or nary storage model, was the storage model for row-oriented databases. NSM stored all n attributes of a record together on disk, and were generally considered the best approach for database systems.\cite{copeland1985decomposition} Motivation towards a column-oriented database began in the 1970s when medical researchers were trying to use statistical analysis on patient records to improve care decisions, but were unsatisfied with query performance.\cite{weyl1975modular}  Searching for improved read performance, the medical researchers worked with computer scientists to develop a database bank for patient records that included "transposed" auxiliary data files for efficient retrieval. \cite{weyl1975modular} 10 years later a new model titled the Decomposition Storage Model, or DSM for short, was developed as an alternative to NSM by storing attribute values in clusters on disk and using "surrogates", or primary keys, to explicitly represent a database entity.\cite{copeland1985decomposition} While this model required twice as many writes for updates compared to NSM, the ability to compress column attributes as well as improved cache performance distinguished it from NSM.\cite{copeland1985decomposition} 

DSM introduced researchers to the advantages of vertically partitioned databases, sparking new research towards ways of improving tuple reconstruction and update performance in DSM databases, even at the expense of adding CPU overhead.\cite{ramamurthy2003case, boncz2005monetdb} The clear motivation for column-stores is to build a database that can avoid reading unnecessary data from disk without complex indexes or fine-tuning. In addition, breakthrough performance features of modern CPUs has compelled database developers to design more CPU-conscious databases that harness modern CPU optimizations. \cite{boncz2005monetdb}

\subsection{Overview of Column-Store Architecture}
Column-stores are \textit{read-optimized} databases, marked beyond their vertical disk layout for their highly compressed data on disk and their use of vector processing for pipelining data. \cite{stonebraker2005c, boncz2005monetdb} Column-stores seek to get the best out of lackluster disk bandwidth performance by storing compressed column data, rather than whole tuples, in disk pages. Moreover, it is not uncommon for column-stores to substitute slotted pages Ð a practice that is useful when updating tuples Ð for larger, denser pages since all column data will be fixed size and compressed. \cite{harizopoulos2006performance} All of these measures allow column-stores to read in more data at once and only those attributes of each record that the query needs. 

Row-stores are not new to compression: they have benefitted from compression techniques such as Dictionary Encoding in reducing the size of tuples on disk.\cite{abadi2006integrating} So what makes column-store compression distinct? One of the major benefits of column-store databases is that all the data on a page belong to the same column, and are therefore highly likely to have strong similarities to physically adjacent data, allowing for a wider variety of compression algorithms to choose from which take advantage of homogenous data.\cite{abadi2006integrating} This reduces the distance between data on disk and reduces the amount of data passed to memory, improving overall system throughput.\cite{abadi2006integrating} Compressing data provides more space for the database system to store copies in different sort orders, providing the query optimizer with more access-plan options for each query.\cite{stonebraker2005c} While compression and decompression require additional CPU overhead, the trade-off for reduced IO time has been found to outweigh the cost of decompression and improve performance.\cite{boncz2005monetdb, abadi2006integrating}Another key aspect of column-stores are their unique query executors that are designed to operate directly on compressed data and take advantage of modern CPU SIMD instructions allowing for heavy loop-pipelining optimizations. \cite{boncz2005monetdb, abadi2006integrating}

The astute reader may have already realized that updates on a column-store must have to access each column separately to update a single record, which pales in comparison to row-stores. The major disadvantage of column-stores is their slow update performance and complex tuple reconstruction process, making them poor options for OLTP databases which have update-heavy workloads.\cite{boncz2005monetdb} Popular open-source column-store "C-store"\cite{stonebraker2005c} introduced a novel solution to the write performance problem by including an additional bulk tuple loader named a write-store into the database system that is optimized for such procedures. In TPC-H benchmarks, column-stores have been shown to outperform row-stores\cite{stonebraker2005c, abadi2008column}, yet there are few caveats to column-stores that deserve mentioning. One caveat is that column-store performance steadily decreases with each additional column that needs to be accessed, even to the point where column-stores are outperformed by row-stores if a majority of a tables' columns must be accessed. \cite{harizopoulos2006performance} Moreover, even if a query only has to access a few columns, it may still wish to project entire tuples. Just as column-stores are poor at updating, they are also poor at reconstructing tuples from multiple columns.\cite{harizopoulos2006performance}

\subsection{Applications}
If column-stores can't promise to outperform row-stores given any read-intensive query, in what situations would a column-store be beneficial? Imagine a retail company has a 10 million row, 100 column fact table that stores all the transactions from different stores, and they have business analysts who want to get information such as how many of product X were purchased in comparison to Product Y, or the total number of purchases above \$100 that happened over the weekend. Now, the queries that can answer these questions will likely only need data from 10 of the 100 columns to execute the query. In a row-store, we would have to read in all attribute values for a tuple, wasting disk bandwidth on irrelevant data. In a column-store, the amount of irrelevant data read in is minimized by only reading the column data relevant to the query. The main idea behind this example is to show how column-stores are a strong match for ad-hoc queries that only need to touch a few columns of a massive database. The column-store architecture has clear commercial value as more and more enterprises entered the data-mining and business analysis markets.\cite{holsheimer1994architectural} Data centers and corporations are interested in improving the efficiency of read scans because they run data-warehousing and business intelligence applications that read large fact-tables to gather analytics. \cite{harizopoulos2006performance} As previously noted, medical researchers often depend on data analysis of medical records for making better diagnosis, and can easily benefit from a technology that allows them faster access to information that will help them treat patients better. \cite{weyl1975modular}

\subsection{Vendor Analysis}
Vendors such as Sybase IQ, MonetDB, VectorWise, Vertica, and Kx Systems have developed different approaches towards implementing column-store databases. Many of these vendors claim to be built "from the ground up", with some boasting benchmark results showing an impressive 270x query response time speedup.\cite{boncz2005monetdb, verticawp} They primarily target business intelligence and decision support applications, focused on providing low latency analytics over massive datasets.\cite{boncz2005monetdb, verticawp, kxwp, vwwp} The following is an overview of each vendor.

\subsubsection{SybaseIQ}
Sybase IQ was one of the first commercial column-store database systems on the market, designed primarily for queries over massive fact-tables that touch few columns but many rows.\cite{macnicol2004sybase} For improved disk bandwidth, Sybase IQ increases page size and compresses data in order to fit as much data onto a page as possible.\cite{macnicol2004sybase} SybaseIQ columns can have multiple indexes because data is amortized through compression, allowing the query executor a larger search space for calculating access plans. Like other column-stores, SybaseIQ tries to choose the best compression scheme for the database, but always to compress the index records into segmented bitmaps, which are used by the query engine to quickly resolve where clauses using boolean operators.\cite{macnicol2004sybase}

\subsubsection{MonetDB}
MonetDB is a vertically fragmented database that focuses on improving CPU efficiency by taking advantage of optimizations such as loop pipelining.\cite{boncz2005monetdb}  MonetDB's key characteristics are its use of lightweight data compression, late materialization, and vector processing.\cite{boncz2005monetdb, vldbtut} MonetDB claims that the lack of IPC efficiency in row-stores is due to extended query expression calculation and lack of loop pipeline.\cite{boncz2005monetdb} Columns are represented on disk using Binary Association Tables (BAT), which are two-column tables with the first column representing the \textit{oid}, or object id, and the second column is the attribute value. The X100 Vectorized Query Processor is MonetDB query engine that is cache-conscious, storing data vertically in the cache in order to take advantage of the loop-pipelining compiler optimizations.\cite{boncz2005monetdb}

\subsubsection{Vertica}
Vertica is the commercial version of the open-source C-Store database.\cite{cstorews} Compared to MonetDB and Sybase IQ, Vertica supports a hybrid-store architecture that contains a main-memory write-store optimized for updates and inserts and a read-store optimized for ad-hoc queries. \cite{stonebraker2005c, verticawp} Both the write and read store are column-oriented, allowing for the stores to share one column-oriented query executor. The write-store moves tuples asynchronously to the read-store, providing for decent update and insert performance.\cite{vldbtut} In their technical overview, Vertica claims to have an "aggressive" column compression scheme that on average can achieve a 90\% reduction, but clearly this depends on the type of data stored, its sort order and the number of distinct values it has, ie. its cardinality.\cite{verticawp} Vertica samples column-data in order to determine the best compression scheme in order to provide enough room for multiple "projections", or copies of the column data in different sort orders.\cite{verticawp} Like Sybase, Vertica also uses multiple projections to optimize query execution. \cite{macnicol2004sybase}

\subsubsection{VectorWise}
VectorWise claims to be "unique" because it takes full advantage of CPU performance features overlooked by competitors such as using SIMD (single instruction, multiple data) instructions to process vectorized data, however other column-stores like MonetDB provide similar SIMD support. \cite{vwwp} \cite{vldbtut} In order to increase system throughput, VectorWise isolates all CPUs and caches to avoid memory contention. \cite{vwwp} VectorWise also provides support for row-oriented tables as a disk-conservative alternative for wide-tables with few rows, which can be beneficial for database administrators who want to further tune a VectorWise database.\cite{vwwp} Similar to Vertica, VectorWise includes an in-memory data structure for supporting efficient updates and inserts, called a Positional Delta Tree.\cite{vwwp} The amount of memory allocated for the PDT can also be tuned for performance. Unlike other column-oriented databases, VectorWise supports the use of storage indexes, which provide the minimum and maximum values of each page that helps avoid reading unnecessary pages from disk.\cite{vwwp}

\subsubsection{Kx Systems}
Kx Systems have designed their column-store database kdb+ to gather and mine both historical (disk) and real-time (RAM) data efficiently, ideal for investment traders and risk management.\cite{kxwp} Handling real-time data in a column-store is a challenge due to its suboptimal insert and updates architecture. In order to provide real-time performance, kdb+ maintains an in-memory database of new data.\cite{kxwp} As well, kdb+ is built to support high parallelism across distributed databases, providing core support for UUIDs as well as a publish and subscription mechanism for load-balancing.\cite{kxwp} Unlike other competitors, kdb+ provides their own programming language "q", a vector processing language intended to compliment a column-oriented database.\cite{kxwp}

\subsection{Analysis}
Over time, we should expect to see more mature advances in hybrid row-column stores that fuse the best of both technologies. Key aspects of column-stores such as vectorized processing, columnar layout, and heavy compression have already found their way into in-memory databases such as Exasol's EXASolution, and will likely continue to be adopted into other database management systems that want to improve ad-hoc query support. In fact, database companies such as Teradata have already released an analytic platform named Aster that allows for both row and column oriented tables, signaling a shift towards a commercial hybrid-store technologies. \cite{asterdata} Research has failed to show that row-stores and column-stores have exclusive features, suggesting that an innovative hybrid-store is not impossible. \cite{abadi2008column} The TPC-H benchmarks\cite{tpchbm} show that the in-memory column-store EXASolution outperforms non-clustered VectorWise databases with twice the queries-per-hour, suggesting that column-stores have a strong future within in-memory database systems. The two database types work well since both have been used to power real-time analytic applications and both focus on achieving the best performance out of modern CPUs. 

\subsubsection{Is It Important?}
Column-stores have been beneficial in demonstrating the performance improvements one can get achieve using a domain-specific database solution instead of a general-purpose DBMS. The lack of disk IO speed improvements over the last couple decades has become a massive bottleneck for businesses and corporations that have petabytes of records that they want to gather statistics from. Big Data is arguably one of the most popular trends in computer science today, yet in order for analysts to get the most out of massive datasets, they require technologies like column-store to scan over billions of rows in a reasonable amount of time. When we look back at the TPC-H benchmarks for the 3,000GB and 10,000GB range, we find analysts-driven databases like EXASolution outperforming traditional databases by considerable scales. \cite{tpchbm} For analytic companies, this means they can get more out of their data faster by adopting these technologies.

\subsubsection{Can the benefits be quantified?}
The typical way to calculate the benefits of a column-store is to compare the performance of ad-hod queries on large star-schema datasets (databases with large fact-tables and small dimension tables) between column-stores and on traditional DBMSs. Since column-store technology is a domain-specific solution for read-intensive applications, it makes sense to compare column-stores performance to row-store performance using decision support benchmarks such as TPC-H. That said, you shouldn't abandon your DBMS in favor for a column-store to speed up read performance in your application. There are a few things to keep in mind when choosing a column-store: is your fact table \textit{large} and  \textit{wide}? Do your queries only touch a small portion of your tables columns? Is your data added in real-time, or do you load records in bulk? Research has shown that tuple-width and query selectivity have significant effects on column-store performance depending on their values, marking queries that access over 50\% of a table's columns as having too large of a CPU overhead to compensate for their IO performance. \cite{harizopoulos2006performance}

\subsubsection{Are there mature products based on this technology?}
There are certainly mature column-store databases such as Sybase IQ and MonetDB which have been around for nearly two decades. VectorWise is another mature column-store which has lead most column-stores in TPC-H benchmark performance. \cite{tpchbm} Gartner's 2012 "hype cycle" shows column-store databases reaching a plateau of expectation within the next two years, which means that column-stores are a "demonstrated and effected"\cite{gartnerhype} technology. 

\subsubsection{Should I invest?}
Since disk IO performance is unlikely to dramatically improve in the coming years, column-stores will remain a valuable technology for business intelligence companies or data warehouses that wish to examine all their data quickly. There is considerable room for improvement within the column-store architecture that we can expect to see in the future, such as improved write and update performance time and tuple reconstruction performance. If disk-based column-stores can reach write and update performances that can start to compare to traditional OLTP DBMSs, then a new paradigm in OLTP column-stores will inevitably begin to emerge, opening up a new set of potential column-store customers. Researchers have already began investigating the possibility of using in-memory column-store databases for OLTP applications, predicting that large enterprises will use in-memory column-stores for everything from business transactions to ad-hoc queries. \cite{plattner2009common}
\pagebreak
\bibliographystyle{plain}
\bibliography{part2}
\end{document}